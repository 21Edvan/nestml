# Generated from pynestml/src/main/grammars/org/Tokens.g4 by ANTLR 4.5.1
# encoding: utf-8
from __future__ import print_function
from antlr4 import *
from io import StringIO


def serializedATN():
    with StringIO() as buf:
        buf.write(u"\3\u0430\ud6d1\u8206\uad2d\u4417\uaef1\u8d80\uaadd\2")
        buf.write(u"\13\177\b\1\4\2\t\2\4\3\t\3\4\4\t\4\4\5\t\5\4\6\t\6\4")
        buf.write(u"\7\t\7\4\b\t\b\4\t\t\t\4\n\t\n\3\2\3\2\7\2\30\n\2\f\2")
        buf.write(u"\16\2\33\13\2\3\2\3\2\3\2\3\2\7\2!\n\2\f\2\16\2$\13\2")
        buf.write(u"\3\2\3\2\3\2\3\2\3\2\3\2\3\2\7\2-\n\2\f\2\16\2\60\13")
        buf.write(u"\2\3\2\3\2\3\2\5\2\65\n\2\3\2\3\2\3\3\3\3\3\3\5\3<\n")
        buf.write(u"\3\3\4\3\4\3\4\3\4\3\5\3\5\5\5D\n\5\3\5\3\5\3\5\3\5\3")
        buf.write(u"\6\3\6\3\7\3\7\3\7\3\7\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3")
        buf.write(u"\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\5\bb\n\b\3")
        buf.write(u"\t\3\t\7\tf\n\t\f\t\16\ti\13\t\3\t\5\tl\n\t\3\t\3\t\7")
        buf.write(u"\tp\n\t\f\t\16\ts\13\t\5\tu\n\t\3\n\5\nx\n\n\3\n\7\n")
        buf.write(u"{\n\n\f\n\16\n~\13\n\4\".\2\13\3\3\5\4\7\5\t\6\13\7\r")
        buf.write(u"\b\17\t\21\n\23\13\3\2\b\4\2\f\f\17\17\4\2\13\13\"\"")
        buf.write(u"\3\2\63;\3\2\62;\6\2&&C\\aac|\7\2&&\62;C\\aac|\u008d")
        buf.write(u"\2\3\3\2\2\2\2\5\3\2\2\2\2\7\3\2\2\2\2\t\3\2\2\2\2\13")
        buf.write(u"\3\2\2\2\2\r\3\2\2\2\2\17\3\2\2\2\2\21\3\2\2\2\2\23\3")
        buf.write(u"\2\2\2\3\64\3\2\2\2\5;\3\2\2\2\7=\3\2\2\2\tA\3\2\2\2")
        buf.write(u"\13I\3\2\2\2\rK\3\2\2\2\17a\3\2\2\2\21k\3\2\2\2\23w\3")
        buf.write(u"\2\2\2\25\31\7%\2\2\26\30\n\2\2\2\27\26\3\2\2\2\30\33")
        buf.write(u"\3\2\2\2\31\27\3\2\2\2\31\32\3\2\2\2\32\65\3\2\2\2\33")
        buf.write(u"\31\3\2\2\2\34\35\7\61\2\2\35\36\7,\2\2\36\"\3\2\2\2")
        buf.write(u"\37!\13\2\2\2 \37\3\2\2\2!$\3\2\2\2\"#\3\2\2\2\" \3\2")
        buf.write(u"\2\2#%\3\2\2\2$\"\3\2\2\2%&\7,\2\2&\65\7\61\2\2\'(\7")
        buf.write(u"$\2\2()\7$\2\2)*\7$\2\2*.\3\2\2\2+-\13\2\2\2,+\3\2\2")
        buf.write(u"\2-\60\3\2\2\2./\3\2\2\2.,\3\2\2\2/\61\3\2\2\2\60.\3")
        buf.write(u"\2\2\2\61\62\7$\2\2\62\63\7$\2\2\63\65\7$\2\2\64\25\3")
        buf.write(u"\2\2\2\64\34\3\2\2\2\64\'\3\2\2\2\65\66\3\2\2\2\66\67")
        buf.write(u"\b\2\2\2\67\4\3\2\2\289\7\17\2\29<\7\f\2\2:<\t\2\2\2")
        buf.write(u";8\3\2\2\2;:\3\2\2\2<\6\3\2\2\2=>\t\3\2\2>?\3\2\2\2?")
        buf.write(u"@\b\4\2\2@\b\3\2\2\2AC\7^\2\2BD\7\17\2\2CB\3\2\2\2CD")
        buf.write(u"\3\2\2\2DE\3\2\2\2EF\7\f\2\2FG\3\2\2\2GH\b\5\2\2H\n\3")
        buf.write(u"\2\2\2IJ\7<\2\2J\f\3\2\2\2KL\7g\2\2LM\7p\2\2MN\7f\2\2")
        buf.write(u"N\16\3\2\2\2OP\7v\2\2PQ\7t\2\2QR\7w\2\2Rb\7g\2\2ST\7")
        buf.write(u"V\2\2TU\7t\2\2UV\7w\2\2Vb\7g\2\2WX\7h\2\2XY\7c\2\2YZ")
        buf.write(u"\7n\2\2Z[\7u\2\2[b\7g\2\2\\]\7H\2\2]^\7c\2\2^_\7n\2\2")
        buf.write(u"_`\7u\2\2`b\7g\2\2aO\3\2\2\2aS\3\2\2\2aW\3\2\2\2a\\\3")
        buf.write(u"\2\2\2b\20\3\2\2\2cg\t\4\2\2df\t\5\2\2ed\3\2\2\2fi\3")
        buf.write(u"\2\2\2ge\3\2\2\2gh\3\2\2\2hl\3\2\2\2ig\3\2\2\2jl\7\62")
        buf.write(u"\2\2kc\3\2\2\2kj\3\2\2\2lt\3\2\2\2mq\7\60\2\2np\t\5\2")
        buf.write(u"\2on\3\2\2\2ps\3\2\2\2qo\3\2\2\2qr\3\2\2\2ru\3\2\2\2")
        buf.write(u"sq\3\2\2\2tm\3\2\2\2tu\3\2\2\2u\22\3\2\2\2vx\t\6\2\2")
        buf.write(u"wv\3\2\2\2x|\3\2\2\2y{\t\7\2\2zy\3\2\2\2{~\3\2\2\2|z")
        buf.write(u"\3\2\2\2|}\3\2\2\2}\24\3\2\2\2~|\3\2\2\2\21\2\31\".\64")
        buf.write(u";Cagkqtwz|\3\2\3\2")
        return buf.getvalue()


class Tokens(Lexer):

    atn = ATNDeserializer().deserialize(serializedATN())

    decisionsToDFA = [ DFA(ds, i) for i, ds in enumerate(atn.decisionToState) ]


    SL_COMMENT = 1
    NEWLINE = 2
    WS = 3
    LINE_ESCAPE = 4
    BLOCK_OPEN = 5
    BLOCK_CLOSE = 6
    BOOLEAN_LITERAL = 7
    NUMERIC_LITERAL = 8
    NAME = 9

    modeNames = [ u"DEFAULT_MODE" ]

    literalNames = [ u"<INVALID>",
            u"':'", u"'end'" ]

    symbolicNames = [ u"<INVALID>",
            u"SL_COMMENT", u"NEWLINE", u"WS", u"LINE_ESCAPE", u"BLOCK_OPEN", 
            u"BLOCK_CLOSE", u"BOOLEAN_LITERAL", u"NUMERIC_LITERAL", u"NAME" ]

    ruleNames = [ u"SL_COMMENT", u"NEWLINE", u"WS", u"LINE_ESCAPE", u"BLOCK_OPEN", 
                  u"BLOCK_CLOSE", u"BOOLEAN_LITERAL", u"NUMERIC_LITERAL", 
                  u"NAME" ]

    grammarFileName = u"Tokens.g4"

    def __init__(self, input=None):
        super(Tokens, self).__init__(input)
        self.checkVersion("4.5.1")
        self._interp = LexerATNSimulator(self, self.atn, self.decisionsToDFA, PredictionContextCache())
        self._actions = None
        self._predicates = None


